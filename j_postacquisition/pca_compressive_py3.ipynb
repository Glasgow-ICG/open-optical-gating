{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import array, dot, mean, std, empty, argsort\n",
    "from numpy.linalg import eigh, solve\n",
    "from numpy.random import randn\n",
    "from matplotlib.pyplot import subplots, show\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "def cov(data):\n",
    "    \"\"\"\n",
    "        covariance matrix\n",
    "        note: specifically for mean-centered data\n",
    "    \"\"\"\n",
    "    N = data.shape[1]\n",
    "    C = empty((N, N))\n",
    "    for j in range(N):\n",
    "        C[j, j] = mean(data[:, j] * data[:, j])\n",
    "        for k in range(N):\n",
    "            C[j, k] = C[k, j] = mean(data[:, j] * data[:, k])\n",
    "    return C\n",
    "\n",
    "def pca(data, pc_count = None):\n",
    "    \"\"\"\n",
    "        Principal component analysis using eigenvalues\n",
    "        note: this mean-centers and auto-scales the data (in-place)\n",
    "    \"\"\"\n",
    "    data -= mean(data, 0)\n",
    "    data /= std(data, 0)\n",
    "    C = cov(data)\n",
    "    E, V = eigh(C)\n",
    "    key = argsort(E)[::-1][:pc_count]\n",
    "    E, V = E[key], V[:, key]\n",
    "    U = dot(V.T, data.T).T\n",
    "    return U, E, V\n",
    "\n",
    "\"\"\" test data \"\"\"\n",
    "data = array([randn(8) for k in range(150)])\n",
    "data[:50, 2:4] += 5\n",
    "data[50:, 2:5] += 5\n",
    "\n",
    "\"\"\" visualize \"\"\"\n",
    "trans = pca(data, 3)[0]\n",
    "fig, (ax1, ax2) = subplots(1, 2)\n",
    "ax1.scatter(data[:50, 0], data[:50, 1], c = 'r')\n",
    "ax1.scatter(data[50:, 0], data[50:, 1], c = 'b')\n",
    "ax2.scatter(trans[:50, 0], trans[:50, 1], c = 'r')\n",
    "ax2.scatter(trans[50:, 0], trans[50:, 1], c = 'b')\n",
    "show()\n",
    "print(data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.zeros((50, 3))\n",
    "x = np.arange(50)\n",
    "data[:,0] = 2 * np.cos(0.5*x) + 0.1 * np.sin(x)\n",
    "data[:,1] = 0.3 * np.sin(0.4*x) + 2 * np.cos(0.5*x)\n",
    "data[:,2] = 2 * np.cos(0.5*x) + 3\n",
    "\n",
    "\n",
    "result, e, v = pca(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(result.shape)\n",
    "print(e.shape)\n",
    "print(v.shape)\n",
    "print(e)\n",
    "print(v) # Looks like first row represents amplitudes for each eigenvector, required to reproduce the first row of the input data\n",
    "plt.plot(data[:,0],'.')\n",
    "plt.plot(data[:,1],'.')\n",
    "plt.plot(data[:,2],'.')\n",
    "plt.plot(result[:,0])\n",
    "plt.plot(result[:,1])\n",
    "plt.plot(result[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Alex compressive data\n",
    "from analyze_dataset import *\n",
    "\n",
    "frameRanges = None\n",
    "\n",
    "# Analyze photodiode channels, with 4-point averaging to smooth out 50Hz interference\n",
    "basePath = '/Users/jonny/Movies/Example_compr_sens_datasets/New datasets w jt phase annotation/Darkfield'\n",
    "source='df_fish3_maskA_photodiodetifs_2000to3499'\n",
    "pathFormat = '%s/'+source+'/%06d.tif'\n",
    "(images, averagePeriod) = LoadAllImages(basePath+'/'+source, downsampleFactor=1, periodRange=None, plotAllPeriods=False, cropRect=None, timestampKey='time_processing_started')\n",
    "\n",
    "# Apply rolling 4-point average for the images (to deal with noise on the raw compressive images)\n",
    "if (True):\n",
    "    for i in range(len(images)-3):\n",
    "        images[i].image = (images[i].image + images[i+1].image + images[i+2].image + images[i+3].image) / 4.0\n",
    "\n",
    "# Condense to a simple 2D array for PCA processing\n",
    "signals = np.zeros((len(images)-3, 7))\n",
    "print(images[i].image.shape)\n",
    "for i in range(len(images)-3):\n",
    "    signals[i,:] = images[i].image[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "\n",
    "plt.plot(signals[:,0]+1500,label='0')\n",
    "plt.plot(signals[:,1]+500,label='1')\n",
    "plt.plot(signals[:,2],label='2')\n",
    "plt.plot(signals[:,3]-500,label='3')\n",
    "plt.plot(signals[:,4],label='4')\n",
    "plt.plot(signals[:,5]-1500,label='5')\n",
    "plt.plot(signals[:,6],label='6')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result, e, v = pca(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=500\n",
    "plt.plot(result[:m,0]/6+7,label='0')\n",
    "plt.plot(result[:m,1]/4+5.5,label='1')\n",
    "plt.plot(result[:m,2]/2+4.5,label='2')\n",
    "plt.plot(result[:m,3]+2.8,label='3')\n",
    "plt.plot(result[:m,4]+1.4,label='4')\n",
    "plt.plot(result[:m,5]+0.6,label='5')\n",
    "plt.plot(result[:m,6],label='6')\n",
    "plt.legend()\n",
    "plt.title('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do phase analysis of Alex's compressive data, so we can generate an estimate of a smoothed noise-reduced single period\n",
    "# The intention is to use the phases from the widefield analysis to assign known phases to the photodiode channels\n",
    "\n",
    "from analyze_dataset import *\n",
    "\n",
    "frameRanges = None\n",
    "\n",
    "if True:\n",
    "    # Analyze photodiode channels, with 4-point averaging to smooth out 50Hz interference\n",
    "    # Note that we look for time_processing_started in the photodiode channel. That is just a synthetic timestamp, and I presume that's the key I suggested Alex fills in.\n",
    "    basePath = '/Users/jonny/Movies/Example_compr_sens_datasets/New datasets w jt phase annotation/Darkfield'\n",
    "    (kt1, kp1, globalShiftSolution, res, shifts, sequenceDrifts) = AnalyzeDataset(basePath, frameRanges, ['df_fish3_maskA_photodiodetifs_2000to3499'], periodRange = np.arange(100, 140, 0.2), numSamplesPerPeriod = 200, source='df_fish3_maskA_photodiodetifs_2000to3499', applyDriftCorrection=False, downsampling=1, interpolationDistanceBetweenSequences=4, rollingAverage=True, sourceTimestampKey='time_processing_started', fluorTimestampKey='time_processing_started')\n",
    "    \n",
    "if True:\n",
    "    basePath = '/Users/jonny/Movies/Example_compr_sens_datasets/New datasets w jt phase annotation/Darkfield'\n",
    "    # NOTE that I have set downsampling to 2 for this script, so processing isn't too ridiculously slow.\n",
    "    (kt3, kp3, globalShiftSolution, res, shifts, sequenceDrifts) = AnalyzeDataset(basePath, frameRanges, ['df_fish3_maskA_widefield_800to1399'], periodRange = np.arange(30, 56, 0.1), numSamplesPerPeriod = 80, source='df_fish3_maskA_widefield_800to1399', applyDriftCorrection=False, downsampling=2, interpolationDistanceBetweenSequences=4, sourceTimestampKey='synthetic_timestamp', fluorTimestampKey='synthetic_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the standard phase-assignment code to assign phases to the photodiode channel,\n",
    "# using the phases calculated from the brightfield channel\n",
    "#def AnnotateDatasetUsingBrightfieldPhases(basePath, imageRangeList, fluorFolders=[], source='Brightfield - Prosilica', key='phase_from_offline_sync_analysis'):\n",
    "\n",
    "# images was already populated with the (4-point-smoothed) photodiode signals earlier\n",
    "print(images.size)\n",
    "print('First image', os.path.basename(images[0].path))\n",
    "print('Last image', os.path.basename(images[-1].path))\n",
    "InterpolateForImagePhases(images, kt3/5, kp3)\n",
    "\n",
    "phaseAndValues = np.zeros((images.size,8))\n",
    "for i in range(images.size):\n",
    "    phaseAndValues[i,0] = images[i].phase\n",
    "    phaseAndValues[i,1:8] = images[i].image[:,0]\n",
    "sortedPhaseAndValues = phaseAndValues[np.argsort(phaseAndValues[:,0])]\n",
    "sortedPhaseAndValues = sortedPhaseAndValues[0:1398,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old = plt.rcParams[\"figure.figsize\"]\n",
    "plt.rcParams[\"figure.figsize\"] = (3,6)\n",
    "\n",
    "\n",
    "def runningMeanFast(x, N):\n",
    "#    return np.convolve(x, np.ones((N,))/N)[(N-1):]\n",
    "    # this behaves in a correct circular manner\n",
    "    return scipy.ndimage.filters.convolve1d(x, np.ones((N,))/N, mode='wrap')\n",
    "\n",
    "def GaussianMean(x, w):\n",
    "    # Calculate a Gaussian-weighted mean with a width of w, based on the actual phase\n",
    "    # differences between each sample\n",
    "    # x is an Nx8 matrix, where the first column has the phase values\n",
    "    result = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        # Calculate the Gaussian-weighted value for entry i in the input matrix\n",
    "        # First calculate the weightings\n",
    "        phaseDiffs = np.abs(x[:,0] - x[i,0])\n",
    "        wrapped = np.where(phaseDiffs > np.pi)\n",
    "        phaseDiffs[wrapped] = 2.0 * np.pi - phaseDiffs[wrapped]\n",
    "        weightings = np.exp(-(phaseDiffs/w)**2)\n",
    "        weightings = weightings / np.sum(weightings)\n",
    "        # Now apply these weightings to the photodiode values\n",
    "        for j in range(1, 8):\n",
    "            result[i,j] = np.sum(x[:,j]*weightings)\n",
    "    return result\n",
    "            \n",
    "\n",
    "# Plot the signals to check that they are similar to the original unsmoothed signals.\n",
    "plt.xlim([0, 2*np.pi])\n",
    "plt.ylim([0, 8000])\n",
    "\n",
    "if False:\n",
    "    plt.plot(sortedPhaseAndValues[:,0], runningMeanFast(sortedPhaseAndValues[:,1]+1500, 20))\n",
    "    plt.plot(sortedPhaseAndValues[:,0], runningMeanFast(sortedPhaseAndValues[:,2]+500, 20))\n",
    "    plt.plot(sortedPhaseAndValues[:,0], runningMeanFast(sortedPhaseAndValues[:,3], 20))\n",
    "    plt.plot(sortedPhaseAndValues[:,0], runningMeanFast(sortedPhaseAndValues[:,4]-500, 20))\n",
    "    plt.plot(sortedPhaseAndValues[:,0], runningMeanFast(sortedPhaseAndValues[:,5], 20))\n",
    "    plt.plot(sortedPhaseAndValues[:,0], runningMeanFast(sortedPhaseAndValues[:,6]-1500, 20))\n",
    "    plt.plot(sortedPhaseAndValues[:,0], runningMeanFast(sortedPhaseAndValues[:,7], 20))\n",
    "else:\n",
    "    gaussianSmoothed = GaussianMean(sortedPhaseAndValues, 0.05)#0.09)\n",
    "    plt.plot(gaussianSmoothed[:,0], gaussianSmoothed[:,1]+1500)\n",
    "    plt.plot(gaussianSmoothed[:,0], gaussianSmoothed[:,2]+500)\n",
    "    plt.plot(gaussianSmoothed[:,0], gaussianSmoothed[:,3])\n",
    "    plt.plot(gaussianSmoothed[:,0], gaussianSmoothed[:,4]-500)\n",
    "    plt.plot(gaussianSmoothed[:,0], gaussianSmoothed[:,5])\n",
    "    plt.plot(gaussianSmoothed[:,0], gaussianSmoothed[:,6]-1500)\n",
    "    plt.plot(gaussianSmoothed[:,0], gaussianSmoothed[:,7])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate smoothed profiles for all channels, and do PCA on them\n",
    "if False:\n",
    "    smoothedSignals = np.zeros((sortedPhaseAndValues.shape[0], 7))\n",
    "    for i in range(7):\n",
    "        smoothedSignals[:,i] = runningMeanFast(sortedPhaseAndValues[:,1+i], 20)\n",
    "else:\n",
    "    smoothedSignals = gaussianSmoothed[:,1:8]\n",
    "print(smoothedSignals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result, e, v = pca(smoothedSignals[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=500\n",
    "plt.xlim([0, 20])#2*np.pi])\n",
    "if False:\n",
    "    # Separate out the components for display purposes\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,0]/6+7,label='0')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,1]/4+5.5,label='1')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,2]/2+4.5,label='2')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,3]+2.8,label='3')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,4]+1.4,label='4')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,5]+0.6,label='5')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,6],label='6')\n",
    "else:\n",
    "    # Plot the components on a common axis\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,0],label='0')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,1],label='1')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,2],label='2')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,3],label='3')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,4],label='4')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,5],label='5')\n",
    "    plt.plot(sortedPhaseAndValues[:,0], result[:,6],label='6')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ****** The PCA mean-centers the data with unit std.\n",
    "# What does this mean in terms of the eigenvalues? \n",
    "# I think it means that the PCA results reconstruct this normalized signal, not the original signal.\n",
    "# Confirm that I am correct, by plotting graphs\n",
    "recoveredSignals = np.zeros(result.shape)\n",
    "channelToPlot = 4\n",
    "\n",
    "for channel in range(7):#channelToPlot,channelToPlot+1): # Make sure I set this to 7 in order to reconstruct all channels\n",
    "    #print('channel %d' % channel)\n",
    "    plt.plot(sortedPhaseAndValues[:,channel+1])\n",
    "    for eigenvalue in range(7):# Make sure I set this to 7 in order to reconstruct full signal\n",
    "        recoveredSignals[:,channel] = recoveredSignals[:,channel] + v[channel,eigenvalue]*result[:,eigenvalue]\n",
    "        print(' ampl %d: %f' % (eigenvalue, v[channel,eigenvalue]))\n",
    "        if eigenvalue >= 6:\n",
    "            plt.plot(recoveredSignals[:,channel] * np.std(sortedPhaseAndValues[:,channel+1]) + np.mean(sortedPhaseAndValues[:,channel+1]), linewidth=3, color='red')\n",
    "    plt.show()\n",
    "#plt.plot(recoveredSignals[:,channelToPlot] * np.std(sortedPhaseAndValues[:,channelToPlot+1]) + np.mean(sortedPhaseAndValues[:,channelToPlot+1]))\n",
    "\n",
    "# Visual estimate of how many eigenfunctions are required to reproduce the input signal convincingly\n",
    "# (starting from the first one each time, rather than cherry-picking, of course)\n",
    "# \"Noise\" is the number of channels that are very clearly separated from the noise in this plot\n",
    "# Channel   Good    Near-perfect  Noise\n",
    "# 0         2       3-7           3\n",
    "# 1         3       4             2\n",
    "# 2         4       4             2\n",
    "# 3         3       4+            3\n",
    "# 4         3       5             4\n",
    "# 5         3       3             2\n",
    "# 6         2       2             1-2\n",
    "#\n",
    "# This would seem to suggest that we have about 4 significant independent modes in our signals.\n",
    "# It also suggests that the first and last channel are fairly redundant in their information?\n",
    "#\n",
    "# I still need to work out how to interpret all this information, though.\n",
    "# I think I need some sense of where the \"noise floor\" is, and what that means for the useful information \n",
    "# I can extract from the signals. Thanks to channel #4, it looks like I can definitely say that 4 modes \n",
    "# are above the noise floor, but it would be nice to be a bit more rigourous about this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ***** I need to think about how much of the total energy is represented in each eigenfunction,\n",
    "# and whether it matters whether or not the signals are normalized.\n",
    "# For my SAD, the offset of an individual channel makes no difference.\n",
    "# The scaling (STD) does make a difference in that one higher-amplitude signal will tend to dominate.\n",
    "# However, a low amplitude signal that will still contribute useful information.\n",
    "# I definitely think the noise floor needs to be taken into consideration here, though I am not sure how.\n",
    "\n",
    "# I could state that x% of the energy of the smoothed signals can be reproduced using N principal components.\n",
    "# I think that statement is valid for any ONE of the smoothed signals, regardless of the normalization used.\n",
    "# (although I still need to think more about what numbers in the analysis to look at to be able to make that statement!)\n",
    "# However, I need to think more about how to make such a statement about ALL the signals together, in a meaningful way.\n",
    "\n",
    "# Looking at the reconstruction from the first N eigenfunctions (see above), it does seem to genuinely be the case that\n",
    "# there are subtle but real features in the curves that can only be reproduced if we use all 7 eigenfunctions.\n",
    "print(e)\n",
    "print(np.cumsum(e)/np.sum(e))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
